{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Investigation of Data Aggregation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Term Code', 'College Code', 'Department Code', 'CRN', 'Campus Code',\n",
      "       'Subject Code', 'Course Number', 'Section Number', 'Section Title',\n",
      "       'Instructor 1 ID', 'Instructor 1 First Name', 'Instructor 1 Last Name',\n",
      "       'Question Number', 'Question', 'Mean', 'Median', 'Standard Deviation',\n",
      "       'Department Mean', 'Department Median', 'Similar College Mean',\n",
      "       'Similar College Median', 'College Mean', 'College Median',\n",
      "       'Percent Rank - Department', 'Percent Rank - College', 'Responses',\n",
      "       'Percent #1', 'Percent #2', 'Percent #3', 'Percent #4', 'Percent #5',\n",
      "       'ZScore - College', 'ZScore - College Similar Sections', 'Course Level',\n",
      "       'Section Size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the data into the pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data_sp18.csv\") # Modify to correct data location\n",
    "# print(df.head())\n",
    "print(df.columns)\n",
    "# help(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "15\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "20\n",
      "20\n",
      "35\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "15\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "15\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "25\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "10\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "25\n",
      "5\n",
      "5\n",
      "15\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "15\n",
      "10\n",
      "25\n",
      "10\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "10\n",
      "20\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "15\n",
      "45\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "30\n",
      "5\n",
      "5\n",
      "20\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "20\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "15\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "10\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/data_sp18.csv\") # Modify to correct data location\n",
    "\n",
    "# Initialize the aggregated dataframe by copying the base data frame\n",
    "ag_df = df.copy()\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "ag_df.drop(['Department Code', 'Question Number','Section Number','Responses', 'CRN','Campus Code','Question', 'Mean', 'Median', 'Standard Deviation', 'Department Mean', 'Department Median', 'Similar College Mean', 'College Mean', 'College Median', 'Percent Rank - Department', 'Percent Rank - College', 'Percent #1', 'Percent #2', 'Percent #3', 'Percent #4', 'Percent #5', 'ZScore - College', 'ZScore - College Similar Sections', 'Course Level', 'Section Size', 'Similar College Median'], axis=1, inplace = True)\n",
    "\n",
    "\n",
    "# Add in the columns to be filled with the aggregated values\n",
    "ag_df.insert(3,'Avg Department Rating', 0.0)\n",
    "ag_df.insert(4,'SD Department Rating', 0.0)\n",
    "ag_df.insert(7,'Avg Course Rating', 0.0)\n",
    "ag_df.insert(8,'SD Course Rating', 0.0)\n",
    "ag_df.insert(12, 'Avg Instructor Rating In Section', 0.0)\n",
    "ag_df.insert(13, 'SD Instructor Rating In Section', 0.0)\n",
    "ag_df.insert(14, 'Num Responses', 0)\n",
    "# # Swap the columns Section Number and Section Title\n",
    "# swap_col_order = list(ag_df.columns)\n",
    "# # swap_col_order[6] = 'Instructor ID'\n",
    "# # swap_col_order[9] = 'Section Number'\n",
    "# ag_df = ag_df.reindex(columns = swap_col_order)\n",
    "\n",
    "# Rename the Instructor 1 ID, Instructor 1 First Name, Last Name columns\n",
    "ag_df.rename(columns = {'Section Title':'Course Title', 'Responses':'Num Responses','Instructor 1 ID': 'Instructor ID', 'Instructor 1 First Name':'Instructor First Name', 'Instructor 1 Last Name':'Instructor Last Name'}, inplace= True)\n",
    "\n",
    "# Remove the repeat rows that will occur because we are taking 1-10 question responses down to 1\n",
    "ag_df.drop_duplicates(subset = ag_df.columns.drop('Course Title'), inplace = True)\n",
    "\n",
    "# Now we have the column headings we want\n",
    "# Fill the columns with correct values\n",
    "# print(ag_df.head())\n",
    "# ag_df[(ag_df['Subject Code']=='DSA') & (ag_df['Course Number']==4413)]\n",
    "import sys\n",
    "# Read in the question mappings values from the mappings.yaml\n",
    "import yaml\n",
    "with open('mappings.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    mappings = yaml.safe_load(f)\n",
    "    question_weighting = mappings['Instructor_question_weighting']\n",
    "\n",
    "# Lets fill the average instructor ranking in each section, i.e. the combined rating for each question per section\n",
    "for subject in df['Subject Code'].unique(): # Iterate over all subjects (test case - for subject in ['DSA']:)\n",
    "    for course in df[(df['Subject Code']==subject)]['Course Number'].unique(): # Iterate over courses with the desired subject \n",
    "        for instructor in df[(df['Subject Code']==subject) & (df['Course Number']==course)]['Instructor 1 ID'].unique(): # Iterate over instructors with desired subject and course number\n",
    "            # Modify the subset based on the desired instructor (see section index above)\n",
    "            subset = df[(df['Subject Code']==subject) & (df['Course Number']==course) & (df['Instructor 1 ID']==instructor)]        \n",
    "            if len(subset)!=0: \n",
    "                # Compute the combined mean and standard deviation of the questions\n",
    "                # Input the standard deviation, mean, number of responses, and the question number mapped to the weights for each subject-course-instructor combination\n",
    "#                 print(len(subset['Standard Deviation']))\n",
    "                instructor_mean, instructor_sd = combine_standard_deviations(subset['Standard Deviation'], subset['Mean'], subset['Responses'], subset['Question Number'].map(arg=question_weighting))\n",
    "                \n",
    "                # Set the combined mean and combined sd value into the aggregated dataframe\n",
    "                # Find the row of interest in the aggregated df\n",
    "                ag_df_section_row = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course) & (ag_df['Instructor ID']==instructor)].index.tolist()\n",
    "                if len(ag_df_section_row)!=1:\n",
    "                    print('Aggregated Dataframe contains incorrect number of entries (number entries =' + str(len(ag_df_section_row))+ ') for subject: '+ str(subject)+ ', course: '+ str(course)+ ', and instructor: '+ str(instructor))\n",
    "                else:\n",
    "                    # Fill the Instructor Ratings Columns\n",
    "                    ag_df.at[ag_df_section_row[0], 'Avg Instructor Rating In Section'] = instructor_mean\n",
    "                    ag_df.at[ag_df_section_row[0], 'SD Instructor Rating In Section'] = instructor_sd\n",
    "                    \n",
    "                    # Fill the Num Responses column, based on the minimum number of responses of the group of questions\n",
    "                    ag_df.at[ag_df_section_row[0], 'Num Responses'] = min(list(subset['Responses']))\n",
    "                    \n",
    "        # Back to Course level of tree, now that we've filled out the instructor level info\n",
    "        # Modify the dataframe subset that consists only of the entries with the desired course (see course index above)\n",
    "        # Note that now our subset consists of aggregated data from all instructors within the desired course\n",
    "        subset = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course)]\n",
    "        # Compute the combined mean and standard deviation of all of the instructors within the course\n",
    "        #### IMPORTANT #### NO POPULATION-BASED OR OTHER WEIGHTING USED IN THE CALCULATION OF SD AND AVERAGE COURSE RATING\n",
    "        \n",
    "        course_mean, course_sd = combine_standard_deviations(subset['SD Instructor Rating In Section'], subset['Avg Instructor Rating In Section'], np.ones(len(subset['SD Instructor Rating In Section'])), np.ones(len(subset['SD Instructor Rating In Section'])))\n",
    "        # Find the row of interest in the desired df\n",
    "        ag_df_course_rows = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course)].index.tolist()\n",
    "        # Fill the Course ratings columns\n",
    "        ag_df.at[ag_df_course_rows, 'Avg Course Rating'] = course_mean\n",
    "        ag_df.at[ag_df_course_rows, 'SD Course Rating'] = course_sd\n",
    "        \n",
    "    # Back to Department level of tree, now that we've filled out the instructor and course level info\n",
    "    # Modify the dataframe subset that consists only of the entries with the desired subject(see course index above)\n",
    "    # Note that now our subset consists of aggregated data from all instructors and courses within the desired subject/department\n",
    "    subset = ag_df[(ag_df['Subject Code']==subject)]\n",
    "    # Compute the combined mean and standard deviation of all of the courses within the department\n",
    "    #### IMPORTANT #### NO POPULATION-BASED OR OTHER WEIGHTING USED IN THE CALCULATION OF SD AND AVERAGE COURSE RATING\n",
    "\n",
    "    department_mean, department_sd = combine_standard_deviations(subset['SD Course Rating'], subset['Avg Course Rating'], np.ones(len(subset['Avg Course Rating'])), np.ones(len(subset['Avg Course Rating'])))\n",
    "    # Find the row of interest in the desired df\n",
    "    ag_df_course_rows = ag_df[(ag_df['Subject Code']==subject)].index.tolist()\n",
    "    # Fill the Course ratings columns\n",
    "    ag_df.at[ag_df_course_rows, 'Avg Department Rating'] = department_mean\n",
    "    ag_df.at[ag_df_course_rows, 'SD Department Rating'] = department_sd\n",
    "        \n",
    "        \n",
    "# subject = 'DSA'\n",
    "# course = 4413\n",
    "# section = 995\n",
    "# print(ag_df.columns)\n",
    "# Create the subset based on the desired subject, course, and section\n",
    "# subset = (df[(df['Subject Code']==subject) & (df['Course Number']==course) & (df['Section Number']==section)])\n",
    "\n",
    "# Create 1D lists based on the desired column\n",
    "# ss_mean = subset['Mean'])\n",
    "# ss_sd = list(subset['Standard Deviation'])\n",
    "# ss_qnums = subset['Question Number']\n",
    "# ss_pops = list(subset['Responses'])\n",
    "# ss_weights = list(ss_qnums.map(arg=question_weighting))\n",
    "# # Find the appropriate lists to pass to \n",
    "# ag_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instructor 1 Last Name</th>\n",
       "      <th>Responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Yoon</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Nicholson</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Shehab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Shehab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Shehab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Shehab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Shehab</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Stewart</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instructor 1 Last Name  Responses\n",
       "0          Lakshmivarahan         10\n",
       "1          Lakshmivarahan         10\n",
       "2          Lakshmivarahan         10\n",
       "3          Lakshmivarahan         10\n",
       "4          Lakshmivarahan         10\n",
       "5          Lakshmivarahan          5\n",
       "6          Lakshmivarahan          5\n",
       "7          Lakshmivarahan          5\n",
       "8          Lakshmivarahan          5\n",
       "9          Lakshmivarahan          5\n",
       "10                   Yoon          4\n",
       "11                   Yoon          4\n",
       "12                   Yoon          4\n",
       "13                   Yoon          4\n",
       "14                   Yoon          4\n",
       "15                   Yoon          4\n",
       "16                   Yoon          4\n",
       "17                   Yoon          3\n",
       "18                   Yoon          4\n",
       "19                   Yoon          4\n",
       "20                Stewart         13\n",
       "21                Stewart         13\n",
       "22                Stewart         12\n",
       "23                Stewart         13\n",
       "24                Stewart         13\n",
       "25                Stewart          3\n",
       "26                Stewart          3\n",
       "27                Stewart          3\n",
       "28                Stewart          3\n",
       "29                Stewart          3\n",
       "30              Nicholson         25\n",
       "31              Nicholson         25\n",
       "32              Nicholson         25\n",
       "33              Nicholson         25\n",
       "34              Nicholson         25\n",
       "35              Nicholson         13\n",
       "36              Nicholson         13\n",
       "37              Nicholson         13\n",
       "38              Nicholson         13\n",
       "39              Nicholson         13\n",
       "40              Nicholson          3\n",
       "41              Nicholson          3\n",
       "42              Nicholson          3\n",
       "43              Nicholson          3\n",
       "44              Nicholson          3\n",
       "45         Lakshmivarahan          8\n",
       "46         Lakshmivarahan          8\n",
       "47         Lakshmivarahan          8\n",
       "48         Lakshmivarahan          8\n",
       "49         Lakshmivarahan          8\n",
       "50                 Shehab         11\n",
       "51                 Shehab         11\n",
       "52                 Shehab         11\n",
       "53                 Shehab         11\n",
       "54                 Shehab         11\n",
       "55                Stewart          7\n",
       "56                Stewart          7\n",
       "57                Stewart          7\n",
       "58                Stewart          7\n",
       "59                Stewart          7"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(ag_df.head(20)['Instructor Last Name'])\n",
    "df.head(60)[['Instructor 1 Last Name','Responses']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Term Code', 'College Code', 'Subject Code', 'Avg Department Rating',\n",
       "       'SD Department Rating', 'Course Number', 'Course Title',\n",
       "       'Avg Course Rating', 'SD Course Rating', 'Instructor ID',\n",
       "       'Instructor First Name', 'Instructor Last Name',\n",
       "       'Avg Instructor Rating In Section', 'SD Instructor Rating In Section',\n",
       "       'Num Responses'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_df.columns.drop('Course Title')\n",
    "ag_df.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula that we will use for computing and combining the ratings from the available question data is shown, without weighting, below. The image is available from [here](https://www.researchgate.net/post/How_to_combine_standard_deviations_for_three_groups)\n",
    "\n",
    "![sd_image](https://www.researchgate.net/profile/Twan_Ten_Haaf/post/How_to_combine_standard_deviations_for_three_groups/attachment/59d64a7479197b80779a4bdd/AS%3A475377563901954%401490350251587/download/Grouped_SD.jpg)\n",
    "\n",
    "In the above formula, the i index corresponds to the question number. In addition to the $ n_i$ and $(n-1)_i$ terms, our formula uses a w_i term for the weighting of each question; That is, each question response and standard deviation is weighted *not only* by the number of responses but also by the weight for that specific question. These formulae will be used throughout the data aggregation for combining data for each section and also for combining standard deviations for each course and each department. Also, the implemented formula does not use the $n-1$, but instead just uses n (minor error for n>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_means(mean_list, pop_list, weight_list):\n",
    "    '''\n",
    "    This function takes lists of means, population sizes, and weights for each population, and combines the result into a single mean value.\n",
    "    * All lists must be the same length\n",
    "    '''\n",
    "    import numpy as np\n",
    "    mean_list = np.array(mean_list)\n",
    "    pop_list = np.array(pop_list)\n",
    "    weight_list = np.array(weight_list)\n",
    "    # Check for equal sized lists\n",
    "    if not len(mean_list)==len(pop_list)==len(weight_list):\n",
    "        print('All input lists to the function -- combine_standard_deviations -- must be of the same length.')\n",
    "        # Proceed with the program\n",
    "    # Combine the population-and-weight-modulated means\n",
    "    mean = np.sum(mean_list*pop_list*weight_list)/(np.sum(pop_list*weight_list))\n",
    "    return mean\n",
    "    \n",
    "\n",
    "def combine_standard_deviations(sd_list, mean_list,pop_list, weight_list):\n",
    "    '''\n",
    "    This function will take lists of standard deviations, means, population sizes, and weights for each list unit. The function\n",
    "    will combine the lists to produce a standard deviation for the group, based on the input parameters. Formula for combining the SD taken from the below link:\n",
    "    \n",
    "    https://www.researchgate.net/post/How_to_combine_standard_deviations_for_three_groups\n",
    "    \n",
    "    * All lists must be same length\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # Convert all input lists into numpy arrays\n",
    "    sd_list = np.array(sd_list)\n",
    "    mean_list = np.array(mean_list)\n",
    "    pop_list = np.array(pop_list)\n",
    "    weight_list = np.array(weight_list)\n",
    "    # Check for equal sized lists\n",
    "    if not len(sd_list) == len(mean_list)==len(pop_list)==len(weight_list):\n",
    "        print('All input lists to the function -- combine_standard_deviations -- must be of the same length.')\n",
    "    # Proceed with the program\n",
    "    # Compute the weighted mean of the populations\n",
    "    pop_mean = combine_means(mean_list, pop_list, weight_list)\n",
    "    # Compute the deviance\n",
    "    deviance = np.sum((pop_list)*(weight_list)*(sd_list**2) + (pop_list*weight_list)*(mean_list - pop_mean)**2)\n",
    "    # Compute the standard deviation\n",
    "    sd = np.sqrt(deviance/(np.sum(pop_list*weight_list)))\n",
    "    return pop_mean,sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.666666666666667\n",
      "pop mean is 22.108843537414966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.880297993804273"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(combine_means([2,9], [10,10], [0.5,1]))\n",
    "combine_standard_deviations([4,6],[50,9], [47,100], [1,1])\n",
    "\n",
    "# Validated the above functions for combined sd and combined means with this website, albeit there was No entry or validation for the weighting\n",
    "# https://www.statstodo.com/CombineMeansSDs_Pgm.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN00': 'ENGR', 'EN01': 'AME', 'EN02': 'CH E', 'EN03': 'CEES'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "with open('mappings.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    mappings = yaml.safe_load(f)\n",
    "Term_code_dict = mappings['Term_code_dict']\n",
    "College_code_dict = mappings['College_code_dict']\n",
    "Department_code_dict = mappings['Department_code_dict']\n",
    "Department_code_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(np.array(np.array([1,2,3])))\n",
    "\n",
    "a**2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student_reviews",
   "language": "python",
   "name": "student_reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
