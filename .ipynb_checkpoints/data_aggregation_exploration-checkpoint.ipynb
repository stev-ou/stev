{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Investigation of Data Aggregation in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Term Code', 'College Code', 'Department Code', 'CRN', 'Campus Code',\n",
      "       'Subject Code', 'Course Number', 'Section Number', 'Section Title',\n",
      "       'Instructor 1 ID', 'Instructor 1 First Name', 'Instructor 1 Last Name',\n",
      "       'Question Number', 'Question', 'Mean', 'Median', 'Standard Deviation',\n",
      "       'Department Mean', 'Department Median', 'Similar College Mean',\n",
      "       'Similar College Median', 'College Mean', 'College Median',\n",
      "       'Percent Rank - Department', 'Percent Rank - College', 'Responses',\n",
      "       'Percent #1', 'Percent #2', 'Percent #3', 'Percent #4', 'Percent #5',\n",
      "       'ZScore - College', 'ZScore - College Similar Sections', 'Course Level',\n",
      "       'Section Size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load the data into the pandas dataframe\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data_sp18.csv\") # Modify to correct data location\n",
    "# print(df.head())\n",
    "print(df.columns)\n",
    "# help(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterated\n",
      "iterated\n",
      "here and 4413\n",
      "iterated\n",
      "iterated\n",
      "here and 5001\n",
      "iterated\n",
      "iterated\n",
      "here and 5011\n",
      "iterated\n",
      "iterated\n",
      "iterated\n",
      "here and 5113\n",
      "iterated\n",
      "here and 5203\n",
      "iterated\n",
      "here and 5900\n",
      "iterated\n",
      "iterated\n",
      "here and 5970\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/data_sp18.csv\") # Modify to correct data location\n",
    "\n",
    "# Initialize the aggregated dataframe by copying the base data frame\n",
    "ag_df = df.copy()\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "ag_df.drop(['Department Code', 'Question Number','Responses', 'CRN','Campus Code','Question', 'Mean', 'Median', 'Standard Deviation', 'Department Mean', 'Department Median', 'Similar College Mean', 'College Mean', 'College Median', 'Percent Rank - Department', 'Percent Rank - College', 'Percent #1', 'Percent #2', 'Percent #3', 'Percent #4', 'Percent #5', 'ZScore - College', 'ZScore - College Similar Sections', 'Course Level', 'Section Size', 'Similar College Median'], axis=1, inplace = True)\n",
    "# print(len(ag_df.columns))\n",
    "# print(ag_df.columns)\n",
    "\n",
    "# Add in the columns to be filled with the aggregated values\n",
    "ag_df.insert(3,'Avg Department Rating', 0.0)\n",
    "ag_df.insert(4,'SD Department Rating', 0.0)\n",
    "ag_df.insert(7,'Avg Course Rating', 0.0)\n",
    "ag_df.insert(8,'SD Course Rating', 0.0)\n",
    "ag_df.insert(13, 'SD Instructor Ranking In Section', 0.0)\n",
    "ag_df.insert(14, 'Avg Instructor Ranking In Section', 0.0)\n",
    "ag_df.insert(15, 'Num Responses', 0)\n",
    "# Swap the columns Section Number and Section Title\n",
    "swap_col_order = list(ag_df.columns)\n",
    "swap_col_order[6] = 'Section Title'\n",
    "swap_col_order[9] = 'Section Number'\n",
    "ag_df = ag_df.reindex(columns = swap_col_order)\n",
    "\n",
    "# Rename the Instructor 1 ID, Instructor 1 First Name, Last Name columns\n",
    "ag_df.rename(columns = {'Section Title':'Course Title', 'Responses':'Num Responses','Instructor 1 ID': 'Instructor ID', 'Instructor 1 First Name':'Instructor First Name', 'Instructor 1 Last Name':'Instructor Last Name'}, inplace= True)\n",
    "\n",
    "# Remove the repeat rows that will occur because we are taking 1-10 question responses down to 1\n",
    "ag_df.drop_duplicates(inplace = True)\n",
    "\n",
    "# Now we have the column headings we want\n",
    "# Fill the columns with correct values\n",
    "# print(ag_df.head())\n",
    "# ag_df[(ag_df['Subject Code']=='DSA') & (ag_df['Course Number']==4413)]\n",
    "import sys\n",
    "# Read in the question mappings values from the mappings.yaml\n",
    "import yaml\n",
    "with open('mappings.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    mappings = yaml.safe_load(f)\n",
    "    question_weighting = mappings['Instructor_question_weighting']\n",
    "\n",
    "# Lets fill the average instructor ranking in each section, i.e. the combined rating for each question per section\n",
    "# for subject in df['Subject Code'].unique():\n",
    "for subject in ['DSA']: # Iterate over all subjects\n",
    "    for course in df[(df['Subject Code']==subject)]['Course Number'].unique(): # Iterate over courses with the desired subject \n",
    "        for section in df[(df['Subject Code']==subject) & (df['Course Number']==course)]['Section Number'].unique(): # Iterate over courses with desired subject and course number\n",
    "            # Modify the subset based on the desired section (see section index above)\n",
    "            subset = df[(df['Subject Code']==subject) & (df['Course Number']==course) & (df['Section Number']==section)]        \n",
    "            if len(subset)!=0: \n",
    "                # Compute the combined mean and standard deviation of the questions\n",
    "                # Input the standard deviation, mean, number of responses, and the question number mapped to the weights for each subject-course-section combination\n",
    "                print('iterated')\n",
    "                section_mean, section_sd = combine_standard_deviations(subset['Standard Deviation'], subset['Mean'], subset['Responses'], subset['Question Number'].map(arg=question_weighting))\n",
    "                \n",
    "                # Set the combined mean and combined sd value into the aggregated dataframe\n",
    "                # Find the row of interest in the desired df\n",
    "                ag_df_section_row = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course) & (ag_df['Section Number']==section)].index.tolist()\n",
    "                if len(ag_df_section_row)!=1:\n",
    "                    sys.error('Aggregated Dataframe does incorrect number of entries (number entries = %d) for subject: '+ str(subject)+ ', course: '+ str(course)+ ', and section: '+ str(section), len(ag_df_row_of_interest))\n",
    "                else:\n",
    "                    # Fill the Instructor Ratings Columns\n",
    "                    ag_df.at[ag_df_section_row[0], 'Avg Instructor Ranking In Section'] = section_mean\n",
    "                    ag_df.at[ag_df_section_row[0], 'SD Instructor Ranking In Section'] = section_sd\n",
    "                    \n",
    "                    # Fill the Num Responses column, based on the minimum number of responses of the group of questions\n",
    "                    ag_df.at[ag_df_section_row[0], 'Num Responses'] = min(list(subset['Responses']))\n",
    "                    \n",
    "        # Back to Course level of tree, now that we've filled out the section level info\n",
    "        print('here and '+str(course))\n",
    "        # Modify the dataframe subset that consists only of the entries with the desired course (see course index above)\n",
    "        # Note that now our subset consists of aggregated data from all sections within the desired course\n",
    "        subset = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course)]\n",
    "        \n",
    "        # Compute the combined mean and standard deviation of all of the sections within the course\n",
    "        #### IMPORTANT #### NO POPULATION-BASED OR OTHER WEIGHTING USED IN THE CALCULATION OF SD AND AVERAGE COURSE RATING\n",
    "        \n",
    "        course_mean, course_sd = combine_standard_deviations(subset['SD Instructor Ranking In Section'], subset['Avg Instructor Ranking In Section'], np.ones(len(subset['SD Instructor Ranking In Section'])), np.ones(len(subset['SD Instructor Ranking In Section'])))\n",
    "        # Find the row of interest in the desired df\n",
    "        ag_df_course_rows = ag_df[(ag_df['Subject Code']==subject) & (ag_df['Course Number']==course)].index.tolist()\n",
    "        # Fill the Course ratings columns\n",
    "        ag_df.at[ag_df_course_rows, 'Avg Course Rating'] = section_mean\n",
    "        ag_df.at[ag_df_course_rows, 'SD Course Rating'] = section_sd\n",
    "        \n",
    "        \n",
    "# subject = 'DSA'\n",
    "# course = 4413\n",
    "# section = 995\n",
    "# print(ag_df.columns)\n",
    "# Create the subset based on the desired subject, course, and section\n",
    "# subset = (df[(df['Subject Code']==subject) & (df['Course Number']==course) & (df['Section Number']==section)])\n",
    "\n",
    "# Create 1D lists based on the desired column\n",
    "# ss_mean = subset['Mean'])\n",
    "# ss_sd = list(subset['Standard Deviation'])\n",
    "# ss_qnums = subset['Question Number']\n",
    "# ss_pops = list(subset['Responses'])\n",
    "# ss_weights = list(ss_qnums.map(arg=question_weighting))\n",
    "# # Find the appropriate lists to pass to \n",
    "# ag_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term Code</th>\n",
       "      <th>College Code</th>\n",
       "      <th>Subject Code</th>\n",
       "      <th>Avg Department Rating</th>\n",
       "      <th>SD Department Rating</th>\n",
       "      <th>Course Number</th>\n",
       "      <th>Course Title</th>\n",
       "      <th>Avg Course Rating</th>\n",
       "      <th>SD Course Rating</th>\n",
       "      <th>Section Number</th>\n",
       "      <th>Instructor ID</th>\n",
       "      <th>Instructor First Name</th>\n",
       "      <th>Instructor Last Name</th>\n",
       "      <th>SD Instructor Ranking In Section</th>\n",
       "      <th>Avg Instructor Ranking In Section</th>\n",
       "      <th>Num Responses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4413</td>\n",
       "      <td>Algorithm Analysis</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>1.060239</td>\n",
       "      <td>995</td>\n",
       "      <td>112111442</td>\n",
       "      <td>S</td>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>0.880424</td>\n",
       "      <td>3.405000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4413</td>\n",
       "      <td>Algorithm Analysis</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>1.060239</td>\n",
       "      <td>996</td>\n",
       "      <td>112111442</td>\n",
       "      <td>S</td>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>1.060239</td>\n",
       "      <td>3.730000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5001</td>\n",
       "      <td>Data Analytics and Media</td>\n",
       "      <td>4.337662</td>\n",
       "      <td>0.644364</td>\n",
       "      <td>995</td>\n",
       "      <td>112129082</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>Yoon</td>\n",
       "      <td>0.814324</td>\n",
       "      <td>4.175000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5001</td>\n",
       "      <td>Data Analytics and Media</td>\n",
       "      <td>4.337662</td>\n",
       "      <td>0.644364</td>\n",
       "      <td>996</td>\n",
       "      <td>112129082</td>\n",
       "      <td>Doyle</td>\n",
       "      <td>Yoon</td>\n",
       "      <td>0.644364</td>\n",
       "      <td>4.337662</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5011</td>\n",
       "      <td>Introduction to R</td>\n",
       "      <td>4.400001</td>\n",
       "      <td>0.785989</td>\n",
       "      <td>995</td>\n",
       "      <td>112979983</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Stewart</td>\n",
       "      <td>1.086889</td>\n",
       "      <td>4.264591</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5011</td>\n",
       "      <td>Introduction to R</td>\n",
       "      <td>4.400001</td>\n",
       "      <td>0.785989</td>\n",
       "      <td>996</td>\n",
       "      <td>112979983</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Stewart</td>\n",
       "      <td>0.785989</td>\n",
       "      <td>4.400001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5113</td>\n",
       "      <td>Analytics &amp; Metaheuristics</td>\n",
       "      <td>4.550001</td>\n",
       "      <td>0.598842</td>\n",
       "      <td>1</td>\n",
       "      <td>112998038</td>\n",
       "      <td>Charles</td>\n",
       "      <td>Nicholson</td>\n",
       "      <td>0.641262</td>\n",
       "      <td>4.584000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5113</td>\n",
       "      <td>Analytics &amp; Metaheuristics</td>\n",
       "      <td>4.550001</td>\n",
       "      <td>0.598842</td>\n",
       "      <td>995</td>\n",
       "      <td>112998038</td>\n",
       "      <td>Charles</td>\n",
       "      <td>Nicholson</td>\n",
       "      <td>0.573953</td>\n",
       "      <td>4.580769</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5113</td>\n",
       "      <td>Analytics &amp; Metaheuristics</td>\n",
       "      <td>4.550001</td>\n",
       "      <td>0.598842</td>\n",
       "      <td>996</td>\n",
       "      <td>112998038</td>\n",
       "      <td>Charles</td>\n",
       "      <td>Nicholson</td>\n",
       "      <td>0.598842</td>\n",
       "      <td>4.550001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5203</td>\n",
       "      <td>Time Series Analysis</td>\n",
       "      <td>2.943750</td>\n",
       "      <td>1.492490</td>\n",
       "      <td>995</td>\n",
       "      <td>112111442</td>\n",
       "      <td>S</td>\n",
       "      <td>Lakshmivarahan</td>\n",
       "      <td>1.492490</td>\n",
       "      <td>2.943750</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5900</td>\n",
       "      <td>Professional Practice</td>\n",
       "      <td>4.754543</td>\n",
       "      <td>0.450802</td>\n",
       "      <td>1</td>\n",
       "      <td>112123488</td>\n",
       "      <td>Randa</td>\n",
       "      <td>Shehab</td>\n",
       "      <td>0.450802</td>\n",
       "      <td>4.754543</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5970</td>\n",
       "      <td>Bayesian Statistics</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>995</td>\n",
       "      <td>112979983</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Stewart</td>\n",
       "      <td>0.643544</td>\n",
       "      <td>4.614288</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>DSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5970</td>\n",
       "      <td>Bayesian Statistics</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>996</td>\n",
       "      <td>112979983</td>\n",
       "      <td>Wayne</td>\n",
       "      <td>Stewart</td>\n",
       "      <td>0.901042</td>\n",
       "      <td>4.475000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1411</td>\n",
       "      <td>Freshman Engineering Exper</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>112281778</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1411</td>\n",
       "      <td>Freshman Engineering Exper</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10</td>\n",
       "      <td>113363969</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Volz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1411</td>\n",
       "      <td>Disc-ENGR 1411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11</td>\n",
       "      <td>112123488</td>\n",
       "      <td>Randa</td>\n",
       "      <td>Shehab</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510</td>\n",
       "      <td>Mep-Orientation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>112364597</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>Morales</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510</td>\n",
       "      <td>Mep-Orientation</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>112364597</td>\n",
       "      <td>Lisa</td>\n",
       "      <td>Morales</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510</td>\n",
       "      <td>Water, H2O &amp; WaTER Intro.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>901</td>\n",
       "      <td>112113508</td>\n",
       "      <td>David</td>\n",
       "      <td>Sabatini</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1510</td>\n",
       "      <td>Water, H2O &amp; WaTER Intro.</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>901</td>\n",
       "      <td>112887816</td>\n",
       "      <td>Jim</td>\n",
       "      <td>Chamberlain</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>112120861</td>\n",
       "      <td>Chad</td>\n",
       "      <td>Davis</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>112281778</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9</td>\n",
       "      <td>112214450</td>\n",
       "      <td>Nils</td>\n",
       "      <td>Gransberg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>112115442</td>\n",
       "      <td>Theresa</td>\n",
       "      <td>Marks</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "      <td>112128340</td>\n",
       "      <td>Thomas</td>\n",
       "      <td>Landers</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26</td>\n",
       "      <td>112125119</td>\n",
       "      <td>Kash</td>\n",
       "      <td>Barker</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27</td>\n",
       "      <td>112128033</td>\n",
       "      <td>Robert</td>\n",
       "      <td>Huck</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>900</td>\n",
       "      <td>113363969</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Volz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>901</td>\n",
       "      <td>112281778</td>\n",
       "      <td>Laura</td>\n",
       "      <td>Wesson</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>201720</td>\n",
       "      <td>EN</td>\n",
       "      <td>ENGR</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2002</td>\n",
       "      <td>Professional Development</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>902</td>\n",
       "      <td>113363969</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Volz</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Term Code College Code Subject Code  Avg Department Rating  \\\n",
       "0       201720           EN          DSA                    0.0   \n",
       "5       201720           EN          DSA                    0.0   \n",
       "10      201720           EN          DSA                    0.0   \n",
       "15      201720           EN          DSA                    0.0   \n",
       "20      201720           EN          DSA                    0.0   \n",
       "25      201720           EN          DSA                    0.0   \n",
       "30      201720           EN          DSA                    0.0   \n",
       "35      201720           EN          DSA                    0.0   \n",
       "40      201720           EN          DSA                    0.0   \n",
       "45      201720           EN          DSA                    0.0   \n",
       "50      201720           EN          DSA                    0.0   \n",
       "55      201720           EN          DSA                    0.0   \n",
       "60      201720           EN          DSA                    0.0   \n",
       "65      201720           EN         ENGR                    0.0   \n",
       "70      201720           EN         ENGR                    0.0   \n",
       "75      201720           EN         ENGR                    0.0   \n",
       "80      201720           EN         ENGR                    0.0   \n",
       "85      201720           EN         ENGR                    0.0   \n",
       "90      201720           EN         ENGR                    0.0   \n",
       "95      201720           EN         ENGR                    0.0   \n",
       "100     201720           EN         ENGR                    0.0   \n",
       "105     201720           EN         ENGR                    0.0   \n",
       "110     201720           EN         ENGR                    0.0   \n",
       "115     201720           EN         ENGR                    0.0   \n",
       "120     201720           EN         ENGR                    0.0   \n",
       "125     201720           EN         ENGR                    0.0   \n",
       "130     201720           EN         ENGR                    0.0   \n",
       "135     201720           EN         ENGR                    0.0   \n",
       "140     201720           EN         ENGR                    0.0   \n",
       "145     201720           EN         ENGR                    0.0   \n",
       "\n",
       "     SD Department Rating  Course Number                Course Title  \\\n",
       "0                     0.0           4413          Algorithm Analysis   \n",
       "5                     0.0           4413          Algorithm Analysis   \n",
       "10                    0.0           5001    Data Analytics and Media   \n",
       "15                    0.0           5001    Data Analytics and Media   \n",
       "20                    0.0           5011           Introduction to R   \n",
       "25                    0.0           5011           Introduction to R   \n",
       "30                    0.0           5113  Analytics & Metaheuristics   \n",
       "35                    0.0           5113  Analytics & Metaheuristics   \n",
       "40                    0.0           5113  Analytics & Metaheuristics   \n",
       "45                    0.0           5203        Time Series Analysis   \n",
       "50                    0.0           5900       Professional Practice   \n",
       "55                    0.0           5970         Bayesian Statistics   \n",
       "60                    0.0           5970         Bayesian Statistics   \n",
       "65                    0.0           1411  Freshman Engineering Exper   \n",
       "70                    0.0           1411  Freshman Engineering Exper   \n",
       "75                    0.0           1411              Disc-ENGR 1411   \n",
       "80                    0.0           1510             Mep-Orientation   \n",
       "85                    0.0           1510             Mep-Orientation   \n",
       "90                    0.0           1510   Water, H2O & WaTER Intro.   \n",
       "95                    0.0           1510   Water, H2O & WaTER Intro.   \n",
       "100                   0.0           2002    Professional Development   \n",
       "105                   0.0           2002    Professional Development   \n",
       "110                   0.0           2002    Professional Development   \n",
       "115                   0.0           2002    Professional Development   \n",
       "120                   0.0           2002    Professional Development   \n",
       "125                   0.0           2002    Professional Development   \n",
       "130                   0.0           2002    Professional Development   \n",
       "135                   0.0           2002    Professional Development   \n",
       "140                   0.0           2002    Professional Development   \n",
       "145                   0.0           2002    Professional Development   \n",
       "\n",
       "     Avg Course Rating  SD Course Rating  Section Number  Instructor ID  \\\n",
       "0             3.730000          1.060239             995      112111442   \n",
       "5             3.730000          1.060239             996      112111442   \n",
       "10            4.337662          0.644364             995      112129082   \n",
       "15            4.337662          0.644364             996      112129082   \n",
       "20            4.400001          0.785989             995      112979983   \n",
       "25            4.400001          0.785989             996      112979983   \n",
       "30            4.550001          0.598842               1      112998038   \n",
       "35            4.550001          0.598842             995      112998038   \n",
       "40            4.550001          0.598842             996      112998038   \n",
       "45            2.943750          1.492490             995      112111442   \n",
       "50            4.754543          0.450802               1      112123488   \n",
       "55            4.475000          0.901042             995      112979983   \n",
       "60            4.475000          0.901042             996      112979983   \n",
       "65            0.000000          0.000000               9      112281778   \n",
       "70            0.000000          0.000000              10      113363969   \n",
       "75            0.000000          0.000000              11      112123488   \n",
       "80            0.000000          0.000000               1      112364597   \n",
       "85            0.000000          0.000000               2      112364597   \n",
       "90            0.000000          0.000000             901      112113508   \n",
       "95            0.000000          0.000000             901      112887816   \n",
       "100           0.000000          0.000000               3      112120861   \n",
       "105           0.000000          0.000000               4      112281778   \n",
       "110           0.000000          0.000000               9      112214450   \n",
       "115           0.000000          0.000000              25      112115442   \n",
       "120           0.000000          0.000000              25      112128340   \n",
       "125           0.000000          0.000000              26      112125119   \n",
       "130           0.000000          0.000000              27      112128033   \n",
       "135           0.000000          0.000000             900      113363969   \n",
       "140           0.000000          0.000000             901      112281778   \n",
       "145           0.000000          0.000000             902      113363969   \n",
       "\n",
       "    Instructor First Name Instructor Last Name  \\\n",
       "0                       S       Lakshmivarahan   \n",
       "5                       S       Lakshmivarahan   \n",
       "10                  Doyle                 Yoon   \n",
       "15                  Doyle                 Yoon   \n",
       "20                  Wayne              Stewart   \n",
       "25                  Wayne              Stewart   \n",
       "30                Charles            Nicholson   \n",
       "35                Charles            Nicholson   \n",
       "40                Charles            Nicholson   \n",
       "45                      S       Lakshmivarahan   \n",
       "50                  Randa               Shehab   \n",
       "55                  Wayne              Stewart   \n",
       "60                  Wayne              Stewart   \n",
       "65                  Laura               Wesson   \n",
       "70                  Kathy                 Volz   \n",
       "75                  Randa               Shehab   \n",
       "80                   Lisa              Morales   \n",
       "85                   Lisa              Morales   \n",
       "90                  David             Sabatini   \n",
       "95                    Jim          Chamberlain   \n",
       "100                  Chad                Davis   \n",
       "105                 Laura               Wesson   \n",
       "110                  Nils            Gransberg   \n",
       "115               Theresa                Marks   \n",
       "120                Thomas              Landers   \n",
       "125                  Kash               Barker   \n",
       "130                Robert                 Huck   \n",
       "135                 Kathy                 Volz   \n",
       "140                 Laura               Wesson   \n",
       "145                 Kathy                 Volz   \n",
       "\n",
       "     SD Instructor Ranking In Section  Avg Instructor Ranking In Section  \\\n",
       "0                            0.880424                           3.405000   \n",
       "5                            1.060239                           3.730000   \n",
       "10                           0.814324                           4.175000   \n",
       "15                           0.644364                           4.337662   \n",
       "20                           1.086889                           4.264591   \n",
       "25                           0.785989                           4.400001   \n",
       "30                           0.641262                           4.584000   \n",
       "35                           0.573953                           4.580769   \n",
       "40                           0.598842                           4.550001   \n",
       "45                           1.492490                           2.943750   \n",
       "50                           0.450802                           4.754543   \n",
       "55                           0.643544                           4.614288   \n",
       "60                           0.901042                           4.475000   \n",
       "65                           0.000000                           0.000000   \n",
       "70                           0.000000                           0.000000   \n",
       "75                           0.000000                           0.000000   \n",
       "80                           0.000000                           0.000000   \n",
       "85                           0.000000                           0.000000   \n",
       "90                           0.000000                           0.000000   \n",
       "95                           0.000000                           0.000000   \n",
       "100                          0.000000                           0.000000   \n",
       "105                          0.000000                           0.000000   \n",
       "110                          0.000000                           0.000000   \n",
       "115                          0.000000                           0.000000   \n",
       "120                          0.000000                           0.000000   \n",
       "125                          0.000000                           0.000000   \n",
       "130                          0.000000                           0.000000   \n",
       "135                          0.000000                           0.000000   \n",
       "140                          0.000000                           0.000000   \n",
       "145                          0.000000                           0.000000   \n",
       "\n",
       "     Num Responses  \n",
       "0               10  \n",
       "5                5  \n",
       "10               4  \n",
       "15               3  \n",
       "20              12  \n",
       "25               3  \n",
       "30              25  \n",
       "35              13  \n",
       "40               3  \n",
       "45               8  \n",
       "50              11  \n",
       "55               7  \n",
       "60               2  \n",
       "65               0  \n",
       "70               0  \n",
       "75               0  \n",
       "80               0  \n",
       "85               0  \n",
       "90               0  \n",
       "95               0  \n",
       "100              0  \n",
       "105              0  \n",
       "110              0  \n",
       "115              0  \n",
       "120              0  \n",
       "125              0  \n",
       "130              0  \n",
       "135              0  \n",
       "140              0  \n",
       "145              0  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula that we will use for computing and combining the ratings from the available question data is shown, without weighting, below. The image is available from [here](https://www.researchgate.net/post/How_to_combine_standard_deviations_for_three_groups)\n",
    "\n",
    "![sd_image](https://www.researchgate.net/profile/Twan_Ten_Haaf/post/How_to_combine_standard_deviations_for_three_groups/attachment/59d64a7479197b80779a4bdd/AS%3A475377563901954%401490350251587/download/Grouped_SD.jpg)\n",
    "\n",
    "In the above formula, the i index corresponds to the question number. In addition to the $ n_i$ and $(n-1)_i$ terms, our formula uses a w_i term for the weighting of each question; That is, each question response and standard deviation is weighted *not only* by the number of responses but also by the weight for that specific question. These formulae will be used throughout the data aggregation for combining data for each section and also for combining standard deviations for each course and each department. Also, the implemented formula does not use the $n-1$, but instead just uses n (minor error for n>5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_means(mean_list, pop_list, weight_list):\n",
    "    '''\n",
    "    This function takes lists of means, population sizes, and weights for each population, and combines the result into a single mean value.\n",
    "    * All lists must be the same length\n",
    "    '''\n",
    "    import numpy as np\n",
    "    mean_list = np.array(mean_list)\n",
    "    pop_list = np.array(pop_list)\n",
    "    weight_list = np.array(weight_list)\n",
    "    # Check for equal sized lists\n",
    "    if not len(mean_list)==len(pop_list)==len(weight_list):\n",
    "        print('All input lists to the function -- combine_standard_deviations -- must be of the same length.')\n",
    "        # Proceed with the program\n",
    "    # Combine the population-and-weight-modulated means\n",
    "    mean = np.sum(mean_list*pop_list*weight_list)/(np.sum(pop_list*weight_list))\n",
    "    return mean\n",
    "    \n",
    "\n",
    "def combine_standard_deviations(sd_list, mean_list,pop_list, weight_list):\n",
    "    '''\n",
    "    This function will take lists of standard deviations, means, population sizes, and weights for each list unit. The function\n",
    "    will combine the lists to produce a standard deviation for the group, based on the input parameters. Formula for combining the SD taken from the below link:\n",
    "    \n",
    "    https://www.researchgate.net/post/How_to_combine_standard_deviations_for_three_groups\n",
    "    \n",
    "    * All lists must be same length\n",
    "    '''\n",
    "    import numpy as np\n",
    "    # Convert all input lists into numpy arrays\n",
    "    sd_list = np.array(sd_list)\n",
    "    mean_list = np.array(mean_list)\n",
    "    pop_list = np.array(pop_list)\n",
    "    weight_list = np.array(weight_list)\n",
    "    # Check for equal sized lists\n",
    "    if not len(sd_list) == len(mean_list)==len(pop_list)==len(weight_list):\n",
    "        print('All input lists to the function -- combine_standard_deviations -- must be of the same length.')\n",
    "    # Proceed with the program\n",
    "    # Compute the weighted mean of the populations\n",
    "    pop_mean = combine_means(mean_list, pop_list, weight_list)\n",
    "    # Compute the deviance\n",
    "    deviance = np.sum((pop_list)*(weight_list)*(sd_list**2) + (pop_list*weight_list)*(mean_list - pop_mean)**2)\n",
    "    # Compute the standard deviation\n",
    "    sd = np.sqrt(deviance/(np.sum(pop_list*weight_list)))\n",
    "    return pop_mean,sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.666666666666667\n",
      "pop mean is 22.108843537414966\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19.880297993804273"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(combine_means([2,9], [10,10], [0.5,1]))\n",
    "combine_standard_deviations([4,6],[50,9], [47,100], [1,1])\n",
    "\n",
    "# Validated the above functions for combined sd and combined means with this website, albeit there was No entry or validation for the weighting\n",
    "# https://www.statstodo.com/CombineMeansSDs_Pgm.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EN00': 'ENGR', 'EN01': 'AME', 'EN02': 'CH E', 'EN03': 'CEES'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "with open('mappings.yaml') as f:\n",
    "    # use safe_load instead load\n",
    "    mappings = yaml.safe_load(f)\n",
    "Term_code_dict = mappings['Term_code_dict']\n",
    "College_code_dict = mappings['College_code_dict']\n",
    "Department_code_dict = mappings['Department_code_dict']\n",
    "Department_code_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array(np.array(np.array([1,2,3])))\n",
    "\n",
    "a**2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "student_reviews",
   "language": "python",
   "name": "student_reviews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
